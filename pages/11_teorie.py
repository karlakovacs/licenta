import streamlit as st

from utils import nav_bar


st.set_page_config(layout="wide", page_title="Teorie", page_icon="ğŸ“š")
nav_bar()

st.title("Modele de ML")

st.markdown(
	r"""
## Logistic Regression (LR)

Pentru o singurÄƒ observaÈ›ie $x$, reprezentatÄƒ ca un vector de caracteristici $[x_1, x_2, \dots, x_n]$, ieÈ™irea clasificatorului $y$ poate fi $1$ (adicÄƒ observaÈ›ia aparÈ›ine clasei) sau $0$ (observaÈ›ia nu aparÈ›ine clasei). Scopul este de a determina probabilitatea $P(y = 1 \mid x)$, adicÄƒ probabilitatea ca observaÈ›ia sÄƒ aparÈ›inÄƒ clasei pozitive.

Regresia logisticÄƒ rezolvÄƒ aceastÄƒ sarcinÄƒ Ã®nvÄƒÈ›Ã¢nd, pe baza unui set de antrenament, un vector de ponderi $w_i$ È™i un termen de bias $b$. Fiecare pondere este un numÄƒr real asociat unei caracteristici de intrare $x_i$ È™i exprimÄƒ importanÈ›a acelei caracteristici Ã®n decizia de clasificare. O pondere pozitivÄƒ oferÄƒ dovezi Ã®n favoarea apartenenÈ›ei la clasa pozitivÄƒ, iar una negativÄƒ â€“ Ã®n favoarea clasei negative. Termenul de bias (sau interceptul) este un alt numÄƒr real care se adaugÄƒ la suma ponderatÄƒ a intrÄƒrilor.

Rezultatul acestei combinaÈ›ii liniare este un numÄƒr real $z$, calculat astfel:
$$z = \sum_{i=1}^{n} w_i x_i + b$$

FuncÈ›ia sigmoidÄƒ este apoi utilizatÄƒ pentru a transforma acest scor $z$ Ã®ntr-o probabilitate:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

FuncÈ›ia sigmoidÄƒ are proprietatea de a mapa orice valoare realÄƒ Ã®n intervalul $(0, 1)$, ceea ce o face potrivitÄƒ pentru estimarea probabilitÄƒÈ›ilor. Astfel, pentru o observaÈ›ie $x$, se poate calcula probabilitatea $P(y = 1 \mid x)$. Clasificarea finalÄƒ este realizatÄƒ aplicÃ¢nd un prag: dacÄƒ $P(y = 1 \mid x) > 0.5$, se considerÄƒ cÄƒ observaÈ›ia aparÈ›ine clasei pozitive; Ã®n caz contrar, aparÈ›ine clasei negative.

Sursa: [Speech and Language Processing. Daniel Jurafsky & James H. Martin. CHAPTER 5: Logistic Regression](https://web.stanford.edu/~jurafsky/slp3/5.pdf)
"""
)

st.markdown(
	r"""
## Linear Discriminant Analysis (LDA)

Analiza discriminantÄƒ liniarÄƒ (LDA) utilizeazÄƒ funcÈ›ii discriminante pentru a clasifica observaÈ›iile Ã®n clase, maximizÃ¢nd variaÈ›ia dintre clase È™i minimizÃ¢nd variaÈ›ia Ã®n interiorul claselor. FuncÈ›ia discriminantÄƒ pentru clasa $k$ Ã®n LDA este derivatÄƒ sub presupunerile cÄƒ datele urmeazÄƒ o distribuÈ›ie normalÄƒ multivariatÄƒ È™i cÄƒ matricile de covarianÈ›Äƒ sunt egale Ã®ntre clase.

FuncÈ›ia discriminantÄƒ LDA pentru clasa $k$ este:
$$\delta_k(x) = x^T \mathbf{\Sigma}^{-1} \mu_k - \frac{1}{2} \mu_k^T \mathbf{\Sigma}^{-1} \mu_k + \log(\pi_k)$$
unde $x$ = vectorul de caracteristici (de intrare), $\mu_k$ = vectorul mediu al clasei k, $\mathbf{\Sigma}$ = matricea de covarianÈ›Äƒ comunÄƒ tuturor claselor, $\pi_k$ = probabilitatea a priori a clasei $k$, ce ia Ã®n considerare dezechilibrul Ã®ntre clase Ã®n stabilirea frontierei de decizie.

O observaÈ›ie $x$ este atribuitÄƒ clasei $k$ care maximizeazÄƒ $\delta_k(x)$. Ãn cazul a douÄƒ clase, frontiera de decizie se reduce la un hiperplan liniar definit de egalitatea $\delta_1(x) = \delta_2(x)$.

[lol]:
- https://www.ibm.com/topics/linear-discriminant-analysis
- https://bookdown.org/tpinto_home/Regression-and-Classification/linear-discriminant-analysis.html
- https://www.engati.com/glossary/linear-discriminant-analysis
"""
)
